{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given Data Loader in Siamese_Sample folder\n",
    "# Train Libraries\n",
    "from __future__ import print_function\n",
    "import argparse, random, copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as T\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "# from dataloader import train_test_split, FingerprintDataset, ImageTransform\n",
    "# from model import SiameseNetwork\n",
    "import json\n",
    "\n",
    "\n",
    "# Model Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "\n",
    "# Dataloader libraries\n",
    "import os\n",
    "import random\n",
    "import albumentations\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import json\n",
    "import os, sys\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_dist = F.pairwise_distance(output1, output2, keepdim=True)\n",
    "\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_dist, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_dist, min=0.0), 2))\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, device, train_loader, optimizer, epoch, log_interval, dry_run):\n",
    "#     model.train()\n",
    "\n",
    "#     criterion = nn.BCEWithLogitsLoss()  # Use BCEWithLogitsLoss for binary classification\n",
    "\n",
    "#     for batch_idx, (images_1, images_2, targets, _, _) in enumerate(train_loader):\n",
    "#         images_1, images_2, targets = images_1.to(device), images_2.to(device), targets.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images_1, images_2)\n",
    "#         targets = targets.view(-1, 1)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         if batch_idx % log_interval == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(images_1), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss.item()))\n",
    "#             if dry_run:\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval, dry_run):\n",
    "    model.train()\n",
    "\n",
    "    # we aren't using `TripletLoss` as the MNIST dataset is simple, so `BCELoss` can do the trick.\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for batch_idx, (images_1, images_2, targets, _, _) in enumerate(train_loader):\n",
    "        images_1, images_2, targets = images_1.to(device), images_2.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images_1, images_2).squeeze()\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(images_1), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, test_batch_size):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    tot_correct = 0\n",
    "    true_correct = 0\n",
    "\n",
    "    tot_test_easy = 0\n",
    "    correct_test_easy = 0\n",
    "\n",
    "    tot_test_med = 0\n",
    "    correct_test_med = 0\n",
    "\n",
    "    tot_test_hard = 0\n",
    "    correct_test_hard = 0\n",
    "\n",
    "    # we aren't using `TripletLoss` as the MNIST dataset is simple, so `BCELoss` can do the trick.\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (images_1, images_2, targets, _, altered_addr) in test_loader:\n",
    "            images_1, images_2, targets = images_1.to(device), images_2.to(device), targets.to(device)\n",
    "            outputs = model(images_1, images_2).squeeze() \n",
    "            test_loss += criterion(outputs, targets).sum().item()  # sum up batch loss\n",
    "            ones = torch.ones(outputs.shape[0],).to(device)\n",
    "            zeros = torch.zeros(outputs.shape[0],).to(device)\n",
    "            pred = torch.where(outputs > 0.5, ones, zeros)  # get the index of the max log-probability\n",
    "            correctPred = pred.eq(targets.view_as(pred)).sum().item()\n",
    "            true_correct += correctPred\n",
    "            for i in range(len(altered_addr)):\n",
    "            # correctPred = pred.eq(targets.view_as(pred)).sum().item()\n",
    "            # print(\"correctPred:\", correctPred)\n",
    "\n",
    "                if \"Easy\" in altered_addr[i]:\n",
    "                    # print(\"EASY\")\n",
    "                    tot_test_easy+=1\n",
    "                    if pred[i]==targets[i]:\n",
    "                        correct_test_easy+=1\n",
    "                        tot_correct+=1\n",
    "\n",
    "                elif \"Medium\" in altered_addr[i]:\n",
    "                    # print(\"MEDIUM\")\n",
    "                    tot_test_med+=1\n",
    "                    if pred[i]==targets[i]:\n",
    "                        correct_test_med+=1\n",
    "                        tot_correct+=1\n",
    "\n",
    "                elif \"Hard\" in altered_addr[i]:\n",
    "                    # print(\"HARD\")\n",
    "                    tot_test_hard+=1\n",
    "                    if pred[i]==targets[i]:\n",
    "                        correct_test_hard+=1\n",
    "                        tot_correct+=1\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # for the 1st epoch, the average loss is 0.0001 and the accuracy 97-98%\n",
    "    # using default settings. After completing the 10th epoch, the average\n",
    "    # loss is 0.0000 and the accuracy 99.5-100% using default settings.\n",
    "    print('\\nTest set: Average loss: {:.4f}, Total Accuracy: {}/{} ({:.0f})%, Easy Accuracy: {}/{} ({:.0f})%, Medium Accuracy: {}/{} ({:.0f})%, Hard Accuracy: {}/{} ({:.0f})%)\\n'.format(\n",
    "        test_loss, tot_correct, len(test_loader.dataset), 100. * tot_correct / len(test_loader.dataset), \n",
    "        correct_test_easy, tot_test_easy, 100 * correct_test_easy / tot_test_easy,\n",
    "        correct_test_med, tot_test_med, 100 * correct_test_med / tot_test_med,\n",
    "        correct_test_hard, tot_test_hard, 100 * correct_test_hard / tot_test_hard,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SiameseNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SiameseNetwork, self).__init__()\n",
    "\n",
    "#         # convolutional neural network\n",
    "#         self.cnn1 = nn.Sequential(\n",
    "#             nn.Conv2d(1, 96, kernel_size=11, stride=4),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(3, stride=2),\n",
    "\n",
    "#             nn.Conv2d(96, 256, kernel_size=5, stride=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "#             nn.Conv2d(256, 384, kernel_size=3, stride=1),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#         # fully connected layers\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(384, 1024),\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             nn.Linear(1024, 256), \n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#         self.fc2 = nn.Linear(256 * 2, 1)  # Concatenate the two feature vectors\n",
    "\n",
    "#         # final linear layer for binary classification\n",
    "    \n",
    "#     def forward_once(self, x):\n",
    "#         output = self.cnn1(x)\n",
    "#         output = output.view(output.size()[0], -1)\n",
    "#         output = self.fc1(output)\n",
    "#         return output\n",
    "    \n",
    "#     def forward(self, input1, input2):\n",
    "#         output1 = self.forward_once(input1)\n",
    "#         output2 = self.forward_once(input2)\n",
    "#         output = torch.cat((output1, output2), dim=1)\n",
    "\n",
    "#         # Pass the concatenated features through the final linear layer\n",
    "#         output = self.fc2(output)\n",
    "\n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SiameseNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SiameseNetwork, self).__init__()\n",
    "\n",
    "#         # convolutional neural network\n",
    "#         self.cnn1 = nn.Sequential(\n",
    "#             nn.Conv2d(1, 96, kernel_size=11, stride=4),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(3, stride=2),\n",
    "\n",
    "#             nn.Conv2d(96, 256, kernel_size=5, stride=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "#             nn.Conv2d(256, 384, kernel_size=3, stride=1),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#         # fully connected layers\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Linear(384, 1024),\n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             nn.Linear(1024, 256), \n",
    "#             nn.ReLU(inplace=True),\n",
    "\n",
    "#             nn.Linear(256, 2)\n",
    "#         )\n",
    "    \n",
    "#     def forward_once(self, x):\n",
    "#         output = self.cnn1(x)\n",
    "#         output = output.view(output.size()[0], -1)\n",
    "#         output = self.fc1(output)\n",
    "#         return output\n",
    "    \n",
    "#     def forward(self, input1, input2):\n",
    "#         output1 = self.forward_once(input1)\n",
    "#         output2 = self.forward_once(input2)\n",
    "#         output = torch.cat((output1, output2), 1)\n",
    "\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class SiameseNetwork(nn.Module):\n",
    "#     \"\"\"\n",
    "#         Siamese network for image similarity estimation.\n",
    "#         The network is composed of two identical networks, one for each input.\n",
    "#         The output of each network is concatenated and passed to a linear layer. \n",
    "#         The output of the linear layer passed through a sigmoid function.\n",
    "#         `\"FaceNet\" <https://arxiv.org/pdf/1503.03832.pdf>`_ is a variant of the Siamese network.\n",
    "#         This implementation varies from FaceNet as we use the `ResNet-18` model from\n",
    "#         `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_ as our feature extractor.\n",
    "#         In addition, we aren't using `TripletLoss` as the MNIST dataset is simple, so `BCELoss` can do the trick.\n",
    "#     \"\"\"\n",
    "#     def __init__(self):\n",
    "#         super(SiameseNetwork, self).__init__()\n",
    "#         # get resnet model\n",
    "#         self.resnet = torchvision.models.resnet50(pretrained=False)\n",
    "\n",
    "#         # over-write the first conv layer to be able to read MNIST images\n",
    "#         # as resnet18 reads (3,x,x) where 3 is RGB channels\n",
    "#         # whereas MNIST has (1,x,x) where 1 is a gray-scale channel\n",
    "#         self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "#         self.fc_in_features = self.resnet.fc.in_features\n",
    "        \n",
    "#         # remove the last layer of resnet18 (linear layer which is before avgpool layer)\n",
    "#         self.resnet = torch.nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
    "#         # print(self.resnet)\n",
    "\n",
    "#         # add linear layers to compare between the features of the two images\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(self.fc_in_features * 2, 256),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(256, 1),\n",
    "#         )\n",
    "\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#         # initialize the weights\n",
    "#         self.resnet.apply(self.init_weights)\n",
    "#         self.fc.apply(self.init_weights)\n",
    "        \n",
    "#     def init_weights(self, m):\n",
    "#         if isinstance(m, nn.Linear):\n",
    "#             torch.nn.init.xavier_uniform(m.weight)\n",
    "#             m.bias.data.fill_(0.01)\n",
    "\n",
    "#     def forward_once(self, x):\n",
    "#         output = self.resnet(x)\n",
    "#         output = output.view(output.size()[0], -1)\n",
    "#         return output\n",
    "\n",
    "#     def forward(self, input1, input2):\n",
    "#         # get two images' features\n",
    "#         output1 = self.forward_once(input1)\n",
    "#         output2 = self.forward_once(input2)\n",
    "\n",
    "#         # concatenate both images' features\n",
    "#         output = torch.cat((output1, output2), 1)\n",
    "\n",
    "#         # pass the concatenation to the linear layers\n",
    "#         output = self.fc(output)\n",
    "\n",
    "#         # pass the out of the linear layers to sigmoid layer\n",
    "#         output = self.sigmoid(output)\n",
    "        \n",
    "#         return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "        Siamese network for image similarity estimation.\n",
    "        The network is composed of two identical networks, one for each input.\n",
    "        The output of each network is concatenated and passed to a linear layer. \n",
    "        The output of the linear layer passed through a sigmoid function.\n",
    "        `\"FaceNet\" <https://arxiv.org/pdf/1503.03832.pdf>`_ is a variant of the Siamese network.\n",
    "        This implementation varies from FaceNet as we use the `ResNet-18` model from\n",
    "        `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_ as our feature extractor.\n",
    "        In addition, we aren't using `TripletLoss` as the MNIST dataset is simple, so `BCELoss` can do the trick.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # get resnet model\n",
    "        self.resnet = torchvision.models.resnet18(pretrained=False)\n",
    "\n",
    "        # over-write the first conv layer to be able to read MNIST images\n",
    "        # as resnet18 reads (3,x,x) where 3 is RGB channels\n",
    "        # whereas MNIST has (1,x,x) where 1 is a gray-scale channel\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.fc_in_features = self.resnet.fc.in_features\n",
    "        \n",
    "        # remove the last layer of resnet18 (linear layer which is after avgpool layer)\n",
    "        print(self.resnet)\n",
    "        self.resnet = torch.nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
    "        # print(self.resnet)\n",
    "\n",
    "        # add linear layers to compare between the features of the two images\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.fc_in_features * 2, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # initialize the weights\n",
    "        self.resnet.apply(self.init_weights)\n",
    "        self.fc.apply(self.init_weights)\n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        output = self.resnet(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # get two images' features\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "\n",
    "        # concatenate both images' features\n",
    "        output = torch.cat((output1, output2), 1)\n",
    "\n",
    "        # pass the concatenation to the linear layers\n",
    "        output = self.fc(output)\n",
    "\n",
    "        # pass the out of the linear layers to sigmoid layer\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split(base_dir, train_ratio, num_people = 600, randomize = True, tests = None):\n",
    "\n",
    "  # Splitting the train & test data according to the 'train_ratio' (ex. 0.9 : 0.1) \n",
    "  train_size = num_people * train_ratio\n",
    "  test_size = num_people - train_size\n",
    "\n",
    "  # Setting train_nums to be a list composed of 1 ~ 600\n",
    "  train_nums = [i+1 for i in range(0,num_people)]\n",
    "  # Setting test_nums to be a list composed of 600 zeros ([0,0,.....,0])\n",
    "  test_nums = [0 for i in range(0, num_people)]\n",
    "\n",
    "  train_list = []\n",
    "  test_list = []\n",
    "\n",
    "  altered_root = os.path.join(base_dir, \"Altered\")\n",
    "  altered_list = [\"Altered-Easy\", \"Altered-Medium\", \"Altered-Hard\"]\n",
    "\n",
    "  test_check = []\n",
    "\n",
    "  # Randomly select people who will be used as test subjects\n",
    "  if randomize :\n",
    "    while len(test_check) < test_size:\n",
    "      test = random.randint(1,num_people)\n",
    "      if test not in test_check :\n",
    "        test_check.append(test)\n",
    "        test_nums[test-1] = test\n",
    "    \n",
    "    for i in range(0, len(train_nums)):\n",
    "      train_nums[i] = train_nums[i]-test_nums[i]\n",
    "\n",
    "    train_nums = set(train_nums)\n",
    "    train_nums.remove(0)\n",
    "    train_nums = list(train_nums)\n",
    "\n",
    "    test_nums = set(test_nums)\n",
    "    test_nums.remove(0)\n",
    "    test_nums = list(test_nums)\n",
    "\n",
    "    print(\"Num Train Data : \",len(train_nums))\n",
    "    print(\"Num Test Data : \", len(test_nums))\n",
    "  # If we pre-select test subjects, then just remove these from the train subjects\n",
    "  # train subjects will initially contain all individuals (1 ~ 600)\n",
    "  else :\n",
    "    test_nums = tests\n",
    "    for i in range(0, len(test_nums)):\n",
    "      if test_nums[i] in train_nums:\n",
    "        train_nums.remove(test_nums[i])\n",
    "  \n",
    "  # Let's go through the altered images and organize them depending on \n",
    "  # whether they can be used for training or testing of the model\n",
    "  for altered in altered_list : \n",
    "    altered_dir = os.path.join(altered_root, altered)\n",
    "    altered_imgs = os.listdir(altered_dir)\n",
    "    \n",
    "    for img in altered_imgs :\n",
    "      person_num = int(img[:img.find('__')])\n",
    "      if person_num in test_nums :\n",
    "        test_img_path = os.path.join(altered_dir, img)\n",
    "        test_list.append(test_img_path)\n",
    "      else:\n",
    "        train_img_path = os.path.join(altered_dir, img)\n",
    "        train_list.append(train_img_path)\n",
    "\n",
    "  # train_list & test_list are made of altered images\n",
    "  return train_nums, train_list, test_nums, test_list\n",
    "\n",
    "class ImageTransform():\n",
    "  def __init__(self, mean, std):\n",
    "    self.data_transform = transforms.Compose([\n",
    "        transforms.Resize((100, 100)),\n",
    "        # transforms.Resize((95, 95)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "  def __call__(self, img):\n",
    "    # print(type(img))\n",
    "    return self.data_transform(img)\n",
    "      \n",
    "class ImageAugTransform_train():\n",
    "  def __init__(self, mean, std):\n",
    "    self.albumentations_transform_oneof = albumentations.Compose([\n",
    "                                                                  albumentations.Resize(100, 100),\n",
    "                                                                  albumentations.Normalize(mean=mean, std=std, max_pixel_value=255),\n",
    "                                                                  albumentations.OneOf([\n",
    "                                                                                        albumentations.HorizontalFlip(p=0.5),\n",
    "                                                                                        albumentations.RandomRotate90(p=0.5)\n",
    "                                                                                        ], p=0.5),\n",
    "                                                                  albumentations.OneOf([\n",
    "                                                                  albumentations.GaussNoise(p=0.5)\n",
    "                                                                  ], p=0.5),\n",
    "                                                                  albumentations.RandomBrightness(limit=0.2, always_apply=False, p=0.5)\n",
    "                                                                  ])\n",
    "\n",
    "  def __call__(self, img):\n",
    "      return self.albumentations_transform_oneof(image=img)\n",
    "\n",
    "# The FingerPrintDataset used to load into pytorch dataloader\n",
    "class FingerprintDataset(Dataset):\n",
    "  def __init__(self, data_nums, data_list, transform=None, transAug=None):\n",
    "    self.data_nums = data_nums\n",
    "    self.data_list = data_list\n",
    "    self.transform = transform\n",
    "    self.transAug = transAug\n",
    "    \n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "  \n",
    "    real_dir = '../../SOCOFing/Real'\n",
    "    real_imgs = os.listdir(real_dir)\n",
    "\n",
    "    # img will be a case from altered image\n",
    "    img = self.data_list[idx]\n",
    "    # find a corresponding name that can be used for identifying the real image\n",
    "    img_name_real = img[img.rfind('/')+1:img.rfind('_')] + '.BMP'\n",
    "    \n",
    "    # This will set probability for creating matching or non-matching case at 50%\n",
    "    print_match = idx % 2 == 0\n",
    "\n",
    "    # This will set probability for applying augmentation at 50%\n",
    "    apply_aug = random.randint(0,1)\n",
    "    \n",
    "    # print_match : 0 means non-matching fingerprints\n",
    "    # print_match : 1 means matching fingerprints\n",
    "    if print_match : # Define the directory for which we will find the matching real case (same individual)\n",
    "      real_img_dir = os.path.join(real_dir, img_name_real)\n",
    "    else :\n",
    "      while True: # Define the directory for which we will find the non-matching real case (different individual)\n",
    "        real_img_num = random.choice(self.data_nums)-1 \n",
    "        img_name_real_unmatch = real_imgs[real_img_num]\n",
    "        \n",
    "        if img_name_real != img_name_real_unmatch:\n",
    "          real_img_dir = os.path.join(real_dir, img_name_real_unmatch)\n",
    "          break\n",
    "\n",
    "    real_directory = real_img_dir\n",
    "    # print(real_directory)\n",
    "    altered_directory = img\n",
    "    # print(altered_directory)\n",
    "    real = Image.open(real_img_dir).convert(\"L\")\n",
    "    altered = Image.open(img).convert(\"L\")\n",
    "    \n",
    "    real = self.transform(real)\n",
    "\n",
    "    if self.transAug is None :\n",
    "      altered = self.transform(altered)\n",
    "    else : \n",
    "      if apply_aug :\n",
    "        altered_np = np.array(altered)\n",
    "        altered_augmented = self.transAug(altered_np)\n",
    "        altered = Image.fromarray(altered_augmented['image'])\n",
    "        altered = self.transform(altered)\n",
    "      else : \n",
    "        altered = self.transform(altered)\n",
    "    \n",
    "    # label = torch.from_numpy(np.array([int(print_match)], dtype=np.long))\n",
    "    label = torch.tensor(print_match, dtype=torch.float)\n",
    "    # label = print_match\n",
    "    # pairset = {\"real\": real, \"altered\": altered, \"label\": torch.from_numpy(np.array([int(print_match)], dtype=np.float32))}\n",
    "    \n",
    "    # return pairset\n",
    "    return real, altered, label, real_directory, altered_directory\n",
    "\n",
    "  def __len__(self):\n",
    "    length = len(self.data_list)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def makedir(path):\n",
    "\tif not os.path.exists(path):\n",
    "\t\tos.makedirs(path)\n",
    "\n",
    "# logs the model you used into a text file. \n",
    "# More than welcome to modify this function to keep track of the results\n",
    "def log():\n",
    "    with open('all_levels_newDataloader.ipynb', 'r') as inputfile:\n",
    "        textstr = inputfile.read()\n",
    "        fn = datetime.now().strftime('%Y_%m_%d_%H_%M_%S') + \".txt\"\n",
    "        log_path = \"./logs/\"\n",
    "        makedir(log_path)\n",
    "        with open(log_path + fn, 'w') as outputfile:\n",
    "            outputfile.write(textstr)\n",
    "        return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_match(model, device, test_loader, tests, transforms):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    real_dir = '../../SOCOFing/Real'\n",
    "    real_imgs = os.listdir(real_dir)\n",
    "    real_test_imgs = []\n",
    "\n",
    "    print(\"bru\")\n",
    "\n",
    "    # for t in tests:\n",
    "    #     for r in real_imgs:\n",
    "    #         if str(t) == r[:r.find('__')]:\n",
    "    #             real_test_imgs.append(r)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (images_1, images_2, targets, _, altered_directory) in enumerate(test_loader):\n",
    "            print(\"here\", idx)\n",
    "            # _, altered, _ = images_1.to(device), images_2.to(device), targets.to(device)\n",
    "            # predicted_probs = {}\n",
    "            # print(altered_directory)\n",
    "            # for real_test_img in real_test_imgs:\n",
    "            #     real_test = Image.open(os.path.join(real_dir, real_test_img)).convert('L')\n",
    "            #     real_test = transforms(real_test).to(device).unsqueeze(0)\n",
    "            #     outputs = model(real_test, altered).squeeze()\n",
    "            #     predicted_probs[real_test_img] = outputs\n",
    "            \n",
    "            # real_test_max_prob = max(predicted_probs, key=predicted_probs.get)\n",
    "            # # print(real_test_max_prob[:-4])\n",
    "            # # print(altered_directory[0][altered_directory[0].rfind('\\\\')+1:altered_directory[0].rfind('_')])\n",
    "            # # assert False\n",
    "            # if real_test_max_prob[:-4] == altered_directory[0][altered_directory[0].rfind('\\\\')+1:altered_directory[0].rfind('_')]:\n",
    "            #     correct += 1\n",
    "            images_1, images_2, targets = images_1.cuda(), images_2.cuda(), targets.cuda()\n",
    "            outputs = model(images_1, images_2).squeeze()\n",
    "            print(\"bruhh\", outputs)\n",
    "            test_loss += criterion(outputs, targets).sum().item()  # sum up batch loss\n",
    "            ones = torch.ones(outputs.shape[0],).to(device)\n",
    "            zeros = torch.zeros(outputs.shape[0],).to(device)\n",
    "            pred = torch.where(outputs > 0.5, ones, zeros)  # get the index of the max log-probability\n",
    "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # for the 1st epoch, the average loss is 0.0001 and the accuracy 97-98%\n",
    "    # using default settings. After completing the 10th epoch, the average\n",
    "    # loss is 0.0000 and the accuracy 99.5-100% using default settings.\n",
    "    print('Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_random_seed(manual_seed):\n",
    "    \"\"\"Init random seed.\"\"\"\n",
    "    seed = None\n",
    "    if manual_seed is None:\n",
    "        seed = random.randint(1, 10000)\n",
    "    else:\n",
    "        seed = manual_seed\n",
    "    print(\"use random seed: {}\".format(seed))\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use random seed: 1001\n",
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "Num Train Data :  420\n",
      "Num Test Data :  180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielsung/miniconda3/envs/SNNLocalEnv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/danielsung/miniconda3/envs/SNNLocalEnv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/54/7k3vmjkx4f7d66glmjs079100000gn/T/ipykernel_32083/1980268194.py:43: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(m.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train List Length :  34410\n",
      "Test List Length :  14860\n",
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/34410 (0%)]\tLoss: 0.812096\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 94\u001b[0m\n\u001b[1;32m     92\u001b[0m scheduler \u001b[39m=\u001b[39m StepLR(optimizer, step_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, gamma\u001b[39m=\u001b[39mgamma)\n\u001b[1;32m     93\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> 94\u001b[0m     train(model, device, train_loader, optimizer, epoch, log_interval, dry_run)\n\u001b[1;32m     95\u001b[0m \u001b[39m# test(model, device, test_loader, test_batch_size)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, log_interval, dry_run)\u001b[0m\n\u001b[1;32m      8\u001b[0m images_1, images_2, targets \u001b[39m=\u001b[39m images_1\u001b[39m.\u001b[39mto(device), images_2\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m outputs \u001b[39m=\u001b[39m model(images_1, images_2)\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     11\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     12\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/SNNLocalEnv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 53\u001b[0m, in \u001b[0;36mSiameseNetwork.forward\u001b[0;34m(self, input1, input2)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input1, input2):\n\u001b[1;32m     52\u001b[0m     \u001b[39m# get two images' features\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     output1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_once(input1)\n\u001b[1;32m     54\u001b[0m     output2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_once(input2)\n\u001b[1;32m     56\u001b[0m     \u001b[39m# concatenate both images' features\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 47\u001b[0m, in \u001b[0;36mSiameseNetwork.forward_once\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_once\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 47\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresnet(x)\n\u001b[1;32m     48\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mview(output\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/SNNLocalEnv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/SNNLocalEnv/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/SNNLocalEnv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/SNNLocalEnv/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/SNNLocalEnv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/SNNLocalEnv/lib/python3.9/site-packages/torchvision/models/resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     90\u001b[0m     identity \u001b[39m=\u001b[39m x\n\u001b[0;32m---> 92\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m     93\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/SNNLocalEnv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/SNNLocalEnv/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/SNNLocalEnv/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "fn = log()\n",
    "# Training settings\n",
    "# parser = argparse.ArgumentParser(description='PyTorch Siamese network Example')\n",
    "# parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "#                     help='input batch size for training (default: 64)')\n",
    "batch_size = 64\n",
    "# parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "#                     help='input batch size for testing (default: 1000)')\n",
    "test_batch_size = 1000\n",
    "# parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "#                     help='number of epochs to train (default: 14)')\n",
    "epochs = 1\n",
    "# parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "#                     help='learning rate (default: 1.0)')\n",
    "lr = 0.9\n",
    "# parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "#                     help='Learning rate step gamma (default: 0.7)')\n",
    "gamma = 0.7\n",
    "# parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "#                     help='disables CUDA training')\n",
    "no_cuda = False\n",
    "# parser.add_argument('--no-mps', action='store_true', default=False,\n",
    "#                     help='disables macOS GPU training')\n",
    "no_mps = False\n",
    "# parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "#                     help='quickly check a single pass')\n",
    "dry_run = False\n",
    "# parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "#                     help='random seed (default: 1)')\n",
    "seed = 1\n",
    "# parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "#                     help='how many batches to wait before logging training status')\n",
    "log_interval = 10\n",
    "# parser.add_argument('--save-model', action='store_true', default=False,\n",
    "#                     help='For Saving the current Model')\n",
    "save_model = True\n",
    "cuda_num = 0\n",
    "# args = parser.parse_args()\n",
    "# use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "# torch.cuda.set_device(cuda_num)\n",
    "init_random_seed(1001)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = SiameseNetwork().to(device)\n",
    "\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs = {'batch_size': test_batch_size}\n",
    "# if use_cuda:\n",
    "#     cuda_kwargs = {'num_workers': 1,\n",
    "#                     'pin_memory': True,\n",
    "#                     'shuffle': True}\n",
    "#     train_kwargs.update(cuda_kwargs)\n",
    "#     test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "\n",
    "mean = (0.5,) #q1\n",
    "std = (0.5,)\n",
    "\n",
    "# Change the directory to where SOCOFing is located. \n",
    "# Direcotories should look as the following\n",
    "# SOCOFing\n",
    "# -- Altered\n",
    "#    -- ...\n",
    "# -- Real\n",
    "#    -- ...\n",
    "base_dir = '../../SOCOFing/'\n",
    "\n",
    "nums = {}\n",
    "train_nums, train_list, test_nums, test_list = train_test_split(base_dir, 0.7)\n",
    "print(\"Train List Length : \", len(train_list))\n",
    "print(\"Test List Length : \", len(test_list))\n",
    "\n",
    "nums['train'] = train_nums\n",
    "nums['test'] = test_nums\n",
    "\n",
    "# state_dict save file name : siamese_network_train%_test%_lr.pt\n",
    "outpath = 'train_test_split_{fn}.json'\n",
    "\n",
    "with open(outpath, 'w') as f:\n",
    "    json.dump(nums, f)\n",
    "\n",
    "\n",
    "train_dataset = FingerprintDataset(train_nums, train_list, transform=ImageTransform(mean,std), transAug=None)\n",
    "test_dataset = FingerprintDataset(test_nums, test_list, transform=ImageTransform(mean,std))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, **test_kwargs)\n",
    "print(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "# optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9, weight_decay = 1e-6) \n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch, log_interval, dry_run)\n",
    "# test(model, device, test_loader, test_batch_size)\n",
    "scheduler.step()\n",
    "# if epoch % 5==0 and save_model:\n",
    "#     # state_dict save file name : siamese_network_train%_test%_lr.pt\n",
    "#     fileName = str(f'Siamese_network_Date:{fn}__Epoch:{epoch}.pt')\n",
    "#     torch.save(model.state_dict(), fileName)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SiameseNetwork' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m SiameseNetwork()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m/home/kiss2023/workspace/Daniel/Siamese/Siamese_network_Date:2023_07_19_14_49_58.txt__Epoch:5.pt\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[39m# test_match(model, device, test_loader, test_nums, ImageTransform(mean, std))\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SiameseNetwork' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/home/kiss2023/workspace/Daniel/Siamese/Siamese_network_Date:2023_07_19_14_49_58.txt__Epoch:5.pt\"))\n",
    "\n",
    "# test_match(model, device, test_loader, test_nums, ImageTransform(mean, std))\n",
    "test(model, device, test_loader, test_batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fingerprint_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
